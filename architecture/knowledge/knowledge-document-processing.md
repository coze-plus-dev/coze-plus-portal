# çŸ¥è¯†åº“æ–‡æ¡£å·¥ç¨‹æŠ€æœ¯è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

æ–‡æ¡£å·¥ç¨‹æ˜¯çŸ¥è¯†åº“ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£å°†ç”¨æˆ·ä¸Šä¼ çš„å„ç§æ ¼å¼æ–‡æ¡£è½¬æ¢ä¸ºå¯æ£€ç´¢çš„ç»“æ„åŒ–æ•°æ®ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æ–‡æ¡£è§£æã€åˆ†å—å¤„ç†ã€å‘é‡åŒ–ç­‰å…³é”®æŠ€æœ¯çš„å®ç°åŸç†å’Œä»£ç å®ç°ã€‚

## ğŸ—ï¸ æ•´ä½“æ¶æ„

### æ–‡æ¡£å¤„ç†æµç¨‹

```mermaid
graph TD
    A[æ–‡æ¡£ä¸Šä¼ ] --> B[æ ¼å¼æ£€æµ‹]
    B --> C[è§£æç­–ç•¥é€‰æ‹©]
    C --> D[å†…å®¹æå–]
    D --> E[æ•°æ®æ¸…æ´—]
    E --> F[å†…å®¹åˆ†å—]
    F --> G[å‘é‡åŒ–]
    G --> H[ç´¢å¼•å­˜å‚¨]
    H --> I[å¤„ç†å®Œæˆ]
```

### æ ¸å¿ƒç»„ä»¶

```
æ–‡æ¡£å¤„ç†å™¨æ¶æ„
â”œâ”€â”€ Parser Manager        # è§£æå™¨ç®¡ç†
â”œâ”€â”€ Document Processor    # æ–‡æ¡£å¤„ç†å™¨
â”œâ”€â”€ Chunking Strategy     # åˆ†å—ç­–ç•¥
â”œâ”€â”€ Event System         # äº‹ä»¶ç³»ç»Ÿ
â””â”€â”€ Progress Tracking    # è¿›åº¦è¿½è¸ª
```

## ğŸ”§ æ–‡æ¡£è§£æå®ç°

### è§£æå™¨æ¥å£è®¾è®¡

**ä½ç½®**: `backend/infra/document/parser/`

```go
// è§£æå™¨ç®¡ç†å™¨æ¥å£
type Manager interface {
    GetParser(config ParseConfig) (Parser, error)
    IsAutoAnnotationSupported() bool
}

// æ–‡æ¡£è§£æå™¨æ¥å£
type Parser interface {
    Parse(ctx context.Context, reader io.Reader) ([]*Document, error)
    GetSupportedExtensions() []FileExtension
}

// è§£æé…ç½®
type ParseConfig struct {
    FileExtension   FileExtension
    DocumentType    DocumentType
    ParsingStrategy *ParsingStrategy
}
```

### æ”¯æŒçš„æ–‡æ¡£ç±»å‹

| æ–‡æ¡£ç±»å‹ | æ–‡ä»¶æ‰©å±•å | è§£æå™¨å®ç° | ç‰¹æ®ŠåŠŸèƒ½ |
|----------|------------|------------|----------|
| **æ–‡æœ¬æ–‡æ¡£** | `.pdf`, `.docx`, `.md`, `.txt` | `builtin.TextParser` | ç»“æ„åŒ–æå–ã€å…ƒæ•°æ®è§£æ |
| **è¡¨æ ¼æ–‡æ¡£** | `.xlsx`, `.csv` | `builtin.TableParser` | å¤šSheetæ”¯æŒã€æ•°æ®ç±»å‹æ¨æ–­ |
| **å›¾ç‰‡æ–‡æ¡£** | `.png`, `.jpg`, `.gif` | `builtin.ImageParser` | OCRè¯†åˆ«ã€æ™ºèƒ½æ ‡æ³¨ |

### è§£æç­–ç•¥é…ç½®

**ä½ç½®**: `backend/domain/knowledge/entity/strategy.go`

```go
// è§£æç­–ç•¥
type ParsingStrategy struct {
    // å›¾ç‰‡è§£æé…ç½®
    CaptionType    *parser.ImageAnnotationType  // æ ‡æ³¨ç±»å‹ï¼šæ‰‹åŠ¨/è‡ªåŠ¨
    
    // PDFè§£æé…ç½®  
    FilterStrategy *FilterStrategy              // è¿‡æ»¤ç­–ç•¥
    
    // è¡¨æ ¼è§£æé…ç½®
    TableSheet     *TableSheet                  // Sheeté…ç½®
}

// å›¾ç‰‡æ ‡æ³¨ç±»å‹
type ImageAnnotationType int32
const (
    ImageAnnotationTypeManual ImageAnnotationType = 1  // æ‰‹åŠ¨æ ‡æ³¨
    ImageAnnotationTypeModel  ImageAnnotationType = 2  // AIè‡ªåŠ¨æ ‡æ³¨
)
```

### æ–‡æ¡£è§£æå®ç°ç»†èŠ‚

**ä½ç½®**: `backend/infra/document/parser/impl/builtin/`

#### PDFæ–‡æ¡£è§£æ

```go
func (p *pdfParser) Parse(ctx context.Context, reader io.Reader) ([]*Document, error) {
    // 1. PDFç»“æ„è§£æ
    doc, err := pdf.ReadFrom(reader)
    if err != nil {
        return nil, err
    }
    
    // 2. æ–‡æœ¬æå–
    var content strings.Builder
    for i := 1; i <= doc.NumPage(); i++ {
        page, err := doc.GetPage(i)
        if err != nil {
            continue
        }
        text, err := page.GetPlainText()
        if err != nil {
            continue
        }
        content.WriteString(text)
    }
    
    // 3. å…ƒæ•°æ®æå–
    metadata := extractPDFMetadata(doc)
    
    return []*Document{{
        Content:  content.String(),
        Metadata: metadata,
    }}, nil
}
```

#### å›¾ç‰‡æ–‡æ¡£è§£æ

```go
func (p *imageParser) Parse(ctx context.Context, reader io.Reader) ([]*Document, error) {
    imageData, err := io.ReadAll(reader)
    if err != nil {
        return nil, err
    }
    
    var content string
    
    // æ ¹æ®è§£æç­–ç•¥é€‰æ‹©å¤„ç†æ–¹å¼
    switch p.config.CaptionType {
    case parser.ImageAnnotationTypeModel:
        // AIè‡ªåŠ¨æ ‡æ³¨
        content, err = p.autoAnnotation(ctx, imageData)
    case parser.ImageAnnotationTypeManual:
        // æ‰‹åŠ¨æ ‡æ³¨ï¼ˆè¿”å›ç©ºå†…å®¹ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼‰
        content = ""
    }
    
    return []*Document{{
        Content: content,
        Type:    DocumentTypeImage,
    }}, err
}

// AIè‡ªåŠ¨æ ‡æ³¨å®ç°
func (p *imageParser) autoAnnotation(ctx context.Context, imageData []byte) (string, error) {
    // è°ƒç”¨è§†è§‰ç†è§£æ¨¡å‹ç”Ÿæˆå›¾ç‰‡æè¿°
    model, err := p.modelFactory.GetVisionModel(ctx)
    if err != nil {
        return "", err
    }
    
    response, err := model.GenerateCaption(ctx, imageData)
    return response.Caption, err
}
```

## âœ‚ï¸ æ–‡æ¡£åˆ†å—å®ç°

### åˆ†å—ç­–ç•¥è®¾è®¡

**ä½ç½®**:
- åˆ†å—ç­–ç•¥ç»“æ„: `backend/domain/knowledge/entity/strategy.go`
- åˆ†å—ç±»å‹å®šä¹‰: `backend/infra/document/parser/manager.go`

```go
// åˆ†å—ç­–ç•¥ (å®šä¹‰åœ¨ entity/strategy.go)
type ChunkingStrategy struct {
    ChunkType       parser.ChunkType  // åˆ†å—ç±»å‹
    ChunkSize       int              // å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    Separator       string           // åˆ†éš”ç¬¦
    Overlap         int              // é‡å å­—ç¬¦æ•°
    TrimSpace       bool             // å»é™¤ç©ºæ ¼
    TrimURLAndEmail bool             // å»é™¤URLå’Œé‚®ç®±
}

// åˆ†å—ç±»å‹ (å®šä¹‰åœ¨ infra/document/parser/manager.go)
type ChunkType int64
const (
    ChunkTypeDefault ChunkType = 0  // é»˜è®¤åˆ†å—ï¼ˆè‡ªåŠ¨ï¼‰
    ChunkTypeCustom  ChunkType = 1  // è‡ªå®šä¹‰åˆ†å—
)
```

### é»˜è®¤åˆ†å—ç­–ç•¥

**ä½ç½®**: `backend/domain/knowledge/service/knowledge.go:929`

```go
func getDefaultChunkStrategy() *entity.ChunkingStrategy {
    return &entity.ChunkingStrategy{
        ChunkType:       parser.ChunkTypeDefault,
        ChunkSize:       consts.DefaultChunkSize,      // 1000å­—ç¬¦
        Separator:       consts.DefaultSeparator,       // "\n\n"
        Overlap:         consts.DefaultOverlap,         // 100å­—ç¬¦
        TrimSpace:       consts.DefaultTrimSpace,       // true
        TrimURLAndEmail: consts.DefaultTrimURLAndEmail, // true
    }
}
```

### åˆ†å—ç®—æ³•å®ç°

#### æ™ºèƒ½è¯­ä¹‰åˆ†å—

```go
func (c *semanticChunker) Chunk(content string) ([]*Chunk, error) {
    // 1. å¥å­åˆ†å‰²
    sentences := c.splitSentences(content)
    
    // 2. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
    similarities := c.calculateSimilarities(sentences)
    
    // 3. è¯­ä¹‰è¾¹ç•Œæ£€æµ‹
    boundaries := c.detectBoundaries(similarities, c.threshold)
    
    // 4. æ„å»ºåˆ†å—
    chunks := make([]*Chunk, 0)
    start := 0
    
    for _, boundary := range boundaries {
        chunkContent := strings.Join(sentences[start:boundary], " ")
        chunks = append(chunks, &Chunk{
            Content:  chunkContent,
            Position: start,
            Size:     len(chunkContent),
        })
        start = boundary
    }
    
    return chunks, nil
}
```

#### å›ºå®šé•¿åº¦åˆ†å—

```go
func (c *fixedChunker) Chunk(content string) ([]*Chunk, error) {
    runes := []rune(content)
    chunks := make([]*Chunk, 0)
    
    for i := 0; i < len(runes); i += c.chunkSize - c.overlap {
        end := i + c.chunkSize
        if end > len(runes) {
            end = len(runes)
        }
        
        chunkContent := string(runes[i:end])
        chunks = append(chunks, &Chunk{
            Content:  chunkContent,
            Position: i,
            Size:     end - i,
        })
        
        if end == len(runes) {
            break
        }
    }
    
    return chunks, nil
}
```

## ğŸš€ æ–‡æ¡£å¤„ç†å™¨å®ç°

### æ–‡æ¡£å¤„ç†å™¨åŸºç±»

**ä½ç½®**: `backend/domain/knowledge/processor/impl/base.go`

```go
type baseDocProcessor struct {
    ctx            context.Context
    UserID         int64
    SpaceID        int64
    Documents      []*entity.Document
    documentSource *entity.DocumentSource
    
    // æ•°æ®åº“æ¨¡å‹
    TableName   string
    docModels   []*model.KnowledgeDocument
    imageSlices []*model.KnowledgeDocumentSlice
    
    // ä¾èµ–æ³¨å…¥
    storage       storage.Storage
    knowledgeRepo repository.KnowledgeRepo
    documentRepo  repository.KnowledgeDocumentRepo
    sliceRepo     repository.KnowledgeDocumentSliceRepo
    idgen         idgen.IDGenerator
    rdb           rdb.RDB
    producer      eventbus.Producer
    parseManager  parser.Manager
}
```

### å¤„ç†æµç¨‹å®ç°

#### 1. æ„å»ºæ•°æ®åº“æ¨¡å‹

```go
func (p *baseDocProcessor) BuildDBModel() error {
    p.docModels = make([]*model.KnowledgeDocument, 0, len(p.Documents))
    
    for i := range p.Documents {
        // ç”Ÿæˆæ–‡æ¡£ID
        id, err := p.idgen.GenID(p.ctx)
        if err != nil {
            return errorx.New(errno.ErrKnowledgeIDGenCode)
        }
        
        // æ„å»ºæ–‡æ¡£æ¨¡å‹
        docModel := &model.KnowledgeDocument{
            ID:            id,
            KnowledgeID:   p.Documents[i].KnowledgeID,
            Name:          p.Documents[i].Name,
            FileExtension: string(p.Documents[i].FileExtension),
            URI:           p.Documents[i].URI,
            DocumentType:  int32(p.Documents[i].Type),
            CreatorID:     p.UserID,
            SpaceID:       p.SpaceID,
            SourceType:    int32(p.Documents[i].Source),
            Status:        int32(knowledge.KnowledgeStatusInit),
            ParseRule: &model.DocumentParseRule{
                ParsingStrategy:  p.Documents[i].ParsingStrategy,
                ChunkingStrategy: p.Documents[i].ChunkingStrategy,
            },
            CreatedAt: time.Now().UnixMilli(),
            UpdatedAt: time.Now().UnixMilli(),
        }
        
        p.Documents[i].ID = docModel.ID
        p.docModels = append(p.docModels, docModel)
    }
    
    return nil
}
```

#### 2. è¡¨æ ¼æ–‡æ¡£ç‰¹æ®Šå¤„ç†

```go
func (p *baseDocProcessor) createTable() error {
    if len(p.Documents) == 1 && p.Documents[0].Type == knowledge.DocumentTypeTable {
        // è¡¨æ ¼çŸ¥è¯†åº“ï¼Œåˆ›å»ºç‰©ç†è¡¨
        rdbColumns := []*rdbEntity.Column{}
        tableColumns := p.Documents[0].TableInfo.Columns
        
        // ç”Ÿæˆåˆ—ID
        columnIDs, err := p.idgen.GenMultiIDs(p.ctx, len(tableColumns)+1)
        if err != nil {
            return errorx.New(errno.ErrKnowledgeIDGenCode)
        }
        
        // æ„å»ºæ•°æ®åº“åˆ—å®šä¹‰
        for i := range tableColumns {
            tableColumns[i].ID = columnIDs[i]
            rdbColumns = append(rdbColumns, &rdbEntity.Column{
                Name:     convert.ColumnIDToRDBField(columnIDs[i]),
                DataType: convert.ConvertColumnType(tableColumns[i].Type),
                NotNull:  tableColumns[i].Indexing,
            })
        }
        
        // æ·»åŠ ä¸»é”®IDåˆ—
        rdbColumns = append(rdbColumns, &rdbEntity.Column{
            Name:     consts.RDBFieldID,
            DataType: rdbEntity.TypeBigInt,
            NotNull:  true,
        })
        
        // åˆ›å»ºç‰©ç†è¡¨
        resp, err := p.rdb.CreateTable(p.ctx, &rdb.CreateTableRequest{
            Table: &rdbEntity.Table{
                Columns: rdbColumns,
                Indexes: []*rdbEntity.Index{
                    {
                        Name:    "pk",
                        Type:    rdbEntity.PrimaryKey,
                        Columns: []string{consts.RDBFieldID},
                    },
                },
            },
        })
        
        if err != nil {
            return errorx.New(errno.ErrKnowledgeCrossDomainCode, errorx.KV("msg", err.Error()))
        }
        
        p.TableName = resp.Table.Name
        p.Documents[0].TableInfo.PhysicalTableName = p.TableName
    }
    
    return nil
}
```

## âš¡ å¼‚æ­¥å‘é‡åŒ–å¤„ç†

### äº‹ä»¶é©±åŠ¨æ¶æ„

**ä½ç½®**: `backend/domain/knowledge/internal/events/`

```go
// æ–‡æ¡£ç´¢å¼•äº‹ä»¶
type IndexDocumentEvent struct {
    KnowledgeID int64              `json:"knowledge_id"`
    Document    *entity.Document   `json:"document"`
}

// æ–‡æ¡£ç‰‡æ®µç´¢å¼•äº‹ä»¶
type IndexSliceEvent struct {
    Slice    *entity.Slice      `json:"slice"`
    Document *entity.Document   `json:"document"`
}

// åˆ é™¤çŸ¥è¯†åº“æ•°æ®äº‹ä»¶
type DeleteKnowledgeDataEvent struct {
    KnowledgeID int64   `json:"knowledge_id"`
    SliceIDs    []int64 `json:"slice_ids"`
}
```

### å¼‚æ­¥å¤„ç†è§¦å‘

**ä½ç½®**: `backend/domain/knowledge/processor/impl/base.go:253`

```go
func (p *baseDocProcessor) Indexing() error {
    // åˆ›å»ºæ–‡æ¡£ç´¢å¼•äº‹ä»¶
    event := events.NewIndexDocumentsEvent(p.Documents[0].KnowledgeID, p.Documents)
    
    // åºåˆ—åŒ–äº‹ä»¶
    body, err := sonic.Marshal(event)
    if err != nil {
        return errorx.New(errno.ErrKnowledgeParseJSONCode, errorx.KV("msg", err.Error()))
    }
    
    // å‘é€åˆ°NSQæ¶ˆæ¯é˜Ÿåˆ—
    if err = p.producer.Send(p.ctx, body); err != nil {
        logs.CtxErrorf(p.ctx, "send message failed, err: %v", err)
        return errorx.New(errno.ErrKnowledgeMQSendFailCode, errorx.KV("msg", err.Error()))
    }
    
    return nil
}
```

### å‘é‡åŒ–å¤„ç†æµç¨‹

```mermaid
graph TD
    A[æ¥æ”¶ç´¢å¼•äº‹ä»¶] --> B[æ–‡æ¡£è§£æ]
    B --> C[å†…å®¹åˆ†å—]
    C --> D[å‘é‡è®¡ç®—]
    D --> E[å‘é‡å­˜å‚¨]
    E --> F[å…¨æ–‡ç´¢å¼•]
    F --> G[æ›´æ–°çŠ¶æ€]
    G --> H[å¤„ç†å®Œæˆ]
```

## ğŸ“Š è¿›åº¦è¿½è¸ªå®ç°

### è¿›åº¦ç¼“å­˜è®¾è®¡

**ä½ç½®**: `backend/infra/document/progressbar/`

```go
type ProgressBar struct {
    documentID int64
    totalSteps int
    cacheCli   cache.Cmdable
    inMemory   bool
}

func (p *ProgressBar) UpdateProgress(ctx context.Context, step int, message string) error {
    percent := float64(step) / float64(p.totalSteps) * 100
    
    progress := ProgressInfo{
        Percent:     percent,
        Step:        step,
        TotalSteps:  p.totalSteps,
        Message:     message,
        UpdatedAt:   time.Now().Unix(),
    }
    
    // ç¼“å­˜è¿›åº¦ä¿¡æ¯
    key := fmt.Sprintf("doc_progress:%d", p.documentID)
    data, _ := json.Marshal(progress)
    
    return p.cacheCli.Set(ctx, key, data, 30*time.Minute).Err()
}
```

### å®æ—¶è¿›åº¦æŸ¥è¯¢

**ä½ç½®**: `backend/domain/knowledge/service/knowledge.go:554`

```go
func (k *knowledgeSVC) getProgressFromCache(ctx context.Context, documentProgress *DocumentProgress) error {
    progressBar := progressbar.NewProgressBar(ctx, documentProgress.ID, 0, k.cacheCli, false)
    percent, remainSec, errMsg := progressBar.GetProgress(ctx)
    
    documentProgress.Progress = int(percent)
    documentProgress.RemainingSec = int64(remainSec)
    
    if len(errMsg) != 0 {
        documentProgress.Status = entity.DocumentStatusFailed
        documentProgress.StatusMsg = errMsg
    }
    
    return nil
}
```

## ğŸ”§ é…ç½®å’Œä¼˜åŒ–

### è§£æå™¨é…ç½®

```go
// è§£æå™¨é…ç½®å¸¸é‡
const (
    DefaultChunkSize        = 1000        // é»˜è®¤åˆ†å—å¤§å°
    DefaultSeparator        = "\n\n"      // é»˜è®¤åˆ†éš”ç¬¦
    DefaultOverlap          = 100         // é»˜è®¤é‡å å­—ç¬¦æ•°
    DefaultTrimSpace        = true        // é»˜è®¤å»é™¤ç©ºæ ¼
    DefaultTrimURLAndEmail  = true        // é»˜è®¤å»é™¤URLå’Œé‚®ç®±
    
    MaxDocumentSize         = 100 * 1024 * 1024  // æœ€å¤§æ–‡æ¡£å¤§å° 100MB
    MaxConcurrentParsing    = 10                 // æœ€å¤§å¹¶å‘è§£ææ•°
)
```

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

1. **å¹¶å‘å¤„ç†**: å¤šä¸ªæ–‡æ¡£å¹¶è¡Œè§£æ
2. **å†…å­˜ä¼˜åŒ–**: æµå¼å¤„ç†å¤§æ–‡ä»¶
3. **ç¼“å­˜æœºåˆ¶**: è§£æç»“æœç¼“å­˜å¤ç”¨
4. **é”™è¯¯é‡è¯•**: è§£æå¤±è´¥è‡ªåŠ¨é‡è¯•
5. **è¿›åº¦åé¦ˆ**: å®æ—¶è¿›åº¦æ›´æ–°

## ğŸš¨ é”™è¯¯å¤„ç†å’Œç›‘æ§

### é”™è¯¯åˆ†ç±»

| é”™è¯¯ç±»å‹ | é”™è¯¯ç  | å¤„ç†ç­–ç•¥ |
|----------|--------|----------|
| æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ | `ErrKnowledgeUnsupportedFormat` | è¿”å›ç”¨æˆ·å‹å¥½æç¤º |
| æ–‡ä»¶è¿‡å¤§ | `ErrKnowledgeFileTooLarge` | å»ºè®®åˆ†å‰²ä¸Šä¼  |
| è§£æå¤±è´¥ | `ErrKnowledgeParseFailCode` | è‡ªåŠ¨é‡è¯•3æ¬¡ |
| å‘é‡åŒ–å¤±è´¥ | `ErrKnowledgeVectorizeFailCode` | é™çº§å¤„ç† |

### ç›‘æ§æŒ‡æ ‡

- æ–‡æ¡£å¤„ç†æˆåŠŸç‡
- å¹³å‡å¤„ç†æ—¶é—´
- å¹¶å‘å¤„ç†æ•°é‡
- é”™è¯¯åˆ†å¸ƒç»Ÿè®¡
- èµ„æºä½¿ç”¨æƒ…å†µ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-27  
**ç›¸å…³æ–‡æ¡£**: [çŸ¥è¯†åº“æ¶æ„æ€»è§ˆ](./knowledge-base-architecture.md)