# çŸ¥è¯†åº“å­˜å‚¨å·¥ç¨‹æŠ€æœ¯è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

å­˜å‚¨å·¥ç¨‹æ˜¯çŸ¥è¯†åº“ç³»ç»Ÿçš„æ•°æ®åº•åº§ï¼Œé‡‡ç”¨åˆ†å±‚å­˜å‚¨æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬å‘é‡æ•°æ®åº“ã€æœç´¢å¼•æ“ã€å¯¹è±¡å­˜å‚¨å’Œå…³ç³»æ•°æ®åº“ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å„å­˜å‚¨ç»„ä»¶çš„æŠ€æœ¯å®ç°ã€é…ç½®ä¼˜åŒ–å’Œè¿ç»´æ–¹æ¡ˆã€‚

## ğŸ—ï¸ å­˜å‚¨æ¶æ„è®¾è®¡

### åˆ†å±‚å­˜å‚¨æ¶æ„

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        A[Knowledge Service]
    end
    
    subgraph "å­˜å‚¨æŠ½è±¡å±‚"
        B[SearchStore Manager]
        C[Storage Interface]
        D[RDB Interface]
    end
    
    subgraph "å­˜å‚¨å®ç°å±‚"
        E[Milvus å‘é‡åº“]
        F[Elasticsearch]
        G[MinIO å¯¹è±¡å­˜å‚¨]
        H[MySQL å…³ç³»æ•°æ®åº“]
        I[Redis ç¼“å­˜]
    end
    
    A --> B
    A --> C
    A --> D
    B --> E
    B --> F
    C --> G
    D --> H
    D --> I
```

### æ•°æ®æµå‘è®¾è®¡

```mermaid
graph TD
    A[åŸå§‹æ–‡æ¡£] --> B[MinIO å¯¹è±¡å­˜å‚¨]
    A --> C[æ–‡æ¡£è§£æ]
    C --> D[ç»“æ„åŒ–æ•°æ®]
    D --> E[MySQL å…ƒæ•°æ®]
    C --> F[æ–‡æœ¬å†…å®¹]
    F --> G[åˆ†å—å¤„ç†]
    G --> H[å‘é‡åŒ–]
    H --> I[Milvus å‘é‡åº“]
    G --> J[Elasticsearch å…¨æ–‡ç´¢å¼•]
    E --> K[Redis ç¼“å­˜]
```

## ğŸ—ƒï¸ å‘é‡æ•°æ®åº“å®ç°

### Milvus é…ç½®

**Docker é…ç½®ä½ç½®**: `docker/docker-compose.yml`

```yaml
milvus-standalone:
  image: milvusdb/milvus:v2.5.10
  container_name: coze-milvus-standalone
  security_opt:
    - seccomp:unconfined
  environment:
    ETCD_ENDPOINTS: etcd:2379
    MINIO_ADDRESS: minio:9000
    MINIO_ACCESS_KEY: minioadmin
    MINIO_SECRET_KEY: minioadmin
  volumes:
    - ./volumes/milvus.yaml:/milvus/configs/milvus.yaml
    - ./data/milvus:/var/lib/milvus
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
    interval: 30s
    start_period: 90s
    timeout: 20s
    retries: 3
  ports:
    - "19530:19530"
    - "9091:9091"
  depends_on:
    - etcd
    - minio
```

### å‘é‡å­˜å‚¨æ¥å£è®¾è®¡

**ä½ç½®**: `backend/infra/document/searchstore/`

```go
// æœç´¢å­˜å‚¨ç®¡ç†å™¨æ¥å£
type Manager interface {
    GetSearchStore(collectionName string) (SearchStore, error)
    CreateCollection(ctx context.Context, req *CreateCollectionRequest) error
    Drop(ctx context.Context, req *DropRequest) error
    ListCollections(ctx context.Context) ([]string, error)
}

// æœç´¢å­˜å‚¨æ¥å£ (ç»§æ‰¿ Eino æ¡†æ¶æ¥å£)
type SearchStore interface {
    indexer.Indexer    // ç´¢å¼•æ¥å£
    retriever.Retriever // æ£€ç´¢æ¥å£
    Delete(ctx context.Context, ids []string) error
}

// é›†åˆåˆ›å»ºè¯·æ±‚
type CreateCollectionRequest struct {
    CollectionName string
    Dimension      int
    MetricType     string
    IndexType      string
    IndexParams    map[string]interface{}
}
```

### å‘é‡ç´¢å¼•ç­–ç•¥

```go
// å‘é‡ç´¢å¼•é…ç½®
type VectorIndexConfig struct {
    // ç´¢å¼•ç±»å‹
    IndexType string `json:"index_type"` // IVF_FLAT, IVF_SQ8, HNSW
    
    // è·ç¦»åº¦é‡
    MetricType string `json:"metric_type"` // L2, IP, COSINE
    
    // ç´¢å¼•å‚æ•°
    IndexParams map[string]interface{} `json:"index_params"`
}

// å¸¸ç”¨ç´¢å¼•é…ç½®
var (
    // HNSW ç´¢å¼• - é«˜ç²¾åº¦åœºæ™¯
    HNSWIndexConfig = VectorIndexConfig{
        IndexType:  "HNSW",
        MetricType: "COSINE",
        IndexParams: map[string]interface{}{
            "M":              16,   // æ„å›¾æ—¶æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
            "efConstruction": 200,  // æ„å›¾æ—¶çš„å€™é€‰åˆ—è¡¨å¤§å°
        },
    }
    
    // IVF_FLAT ç´¢å¼• - å¹³è¡¡åœºæ™¯
    IVFFlatIndexConfig = VectorIndexConfig{
        IndexType:  "IVF_FLAT",
        MetricType: "COSINE", 
        IndexParams: map[string]interface{}{
            "nlist": 1024, // èšç±»ä¸­å¿ƒæ•°é‡
        },
    }
    
    // IVF_SQ8 ç´¢å¼• - å¤§è§„æ¨¡åœºæ™¯
    IVFSQ8IndexConfig = VectorIndexConfig{
        IndexType:  "IVF_SQ8",
        MetricType: "COSINE",
        IndexParams: map[string]interface{}{
            "nlist": 1024,
        },
    }
)
```

### å‘é‡æ•°æ®æ“ä½œ

#### å‘é‡ç´¢å¼•å®ç°

```go
func (m *milvusManager) Index(ctx context.Context, request *indexer.IndexRequest) error {
    collectionName := request.Options["collection_name"].(string)
    
    // 1. è·å–æˆ–åˆ›å»ºé›†åˆ
    collection, err := m.getOrCreateCollection(ctx, collectionName)
    if err != nil {
        return err
    }
    
    // 2. æ„å»ºå‘é‡æ•°æ®
    var vectors [][]float32
    var ids []string
    var metadataList []map[string]interface{}
    
    for _, doc := range request.Documents {
        vectors = append(vectors, doc.Vector)
        ids = append(ids, doc.ID)
        metadataList = append(metadataList, doc.Metadata)
    }
    
    // 3. æ‰¹é‡æ’å…¥å‘é‡
    insertReq := &milvuspb.InsertRequest{
        CollectionName: collectionName,
        FieldsData: []*schemapb.FieldData{
            {
                FieldName: "id",
                Type:      schemapb.DataType_VarChar,
                Field: &schemapb.FieldData_Scalars{
                    Scalars: &schemapb.ScalarField{
                        Data: &schemapb.ScalarField_StringData{
                            StringData: &schemapb.StringArray{Data: ids},
                        },
                    },
                },
            },
            {
                FieldName: "vector",
                Type:      schemapb.DataType_FloatVector,
                Field: &schemapb.FieldData_Vectors{
                    Vectors: &schemapb.VectorField{
                        Data: &schemapb.VectorField_FloatVector{
                            FloatVector: &schemapb.FloatArray{
                                Data: flattenVectors(vectors),
                            },
                        },
                    },
                },
            },
            {
                FieldName: "content",
                Type:      schemapb.DataType_VarChar,
                Field: &schemapb.FieldData_Scalars{
                    Scalars: &schemapb.ScalarField{
                        Data: &schemapb.ScalarField_StringData{
                            StringData: &schemapb.StringArray{
                                Data: extractContent(request.Documents),
                            },
                        },
                    },
                },
            },
        },
    }
    
    _, err = m.client.Insert(ctx, insertReq)
    if err != nil {
        return fmt.Errorf("failed to insert vectors: %w", err)
    }
    
    // 4. åˆ·æ–°é›†åˆç¡®ä¿æ•°æ®å¯è§
    flushReq := &milvuspb.FlushRequest{
        CollectionNames: []string{collectionName},
    }
    
    _, err = m.client.Flush(ctx, flushReq)
    return err
}
```

#### å‘é‡æ£€ç´¢å®ç°

```go
func (m *milvusManager) Retrieve(ctx context.Context, request *retriever.RetrieveRequest) (*retriever.RetrieveResponse, error) {
    collectionName := request.Options["collection_name"].(string)
    topK := request.TopK
    scoreThreshold := request.Options["score_threshold"].(float64)
    
    // 1. æŸ¥è¯¢å‘é‡ç¼–ç 
    queryVector, err := m.encodeQuery(ctx, request.Query)
    if err != nil {
        return nil, err
    }
    
    // 2. æ„å»ºæœç´¢è¯·æ±‚
    searchReq := &milvuspb.SearchRequest{
        CollectionName: collectionName,
        SearchParams: []*commonpb.KeyValuePair{
            {Key: "anns_field", Value: "vector"},
            {Key: "topk", Value: strconv.Itoa(topK)},
            {Key: "params", Value: `{"ef": 64}`}, // HNSW æœç´¢å‚æ•°
            {Key: "metric_type", Value: "COSINE"},
        },
        PlaceholderGroup: buildPlaceholderGroup(queryVector),
        OutputFields:     []string{"id", "content"},
    }
    
    // 3. æ‰§è¡Œå‘é‡æœç´¢
    searchResp, err := m.client.Search(ctx, searchReq)
    if err != nil {
        return nil, fmt.Errorf("vector search failed: %w", err)
    }
    
    // 4. è§£ææœç´¢ç»“æœ
    var documents []*retriever.Document
    for i, result := range searchResp.Results.GetResults() {
        score := result.Scores[i]
        if score < scoreThreshold {
            continue
        }
        
        doc := &retriever.Document{
            ID:      result.Ids.GetStrId().Data[i],
            Content: extractContentFromResult(result, i),
            Score:   score,
            Metadata: map[string]interface{}{
                "collection": collectionName,
                "vector_score": score,
            },
        }
        documents = append(documents, doc)
    }
    
    return &retriever.RetrieveResponse{
        Documents: documents,
    }, nil
}
```

## ğŸ” æœç´¢å¼•æ“å®ç°

### Elasticsearch é…ç½®

**Docker é…ç½®ä½ç½®**: `docker/docker-compose.yml`

```yaml
elasticsearch:
  image: elasticsearch:8.18.0
  container_name: coze-elasticsearch
  restart: always
  environment:
    discovery.type: single-node
    xpack.security.enabled: false
    xpack.security.enrollment.enabled: false
    cluster.routing.allocation.disk.threshold_enabled: false
    bootstrap.memory_lock: true
    ES_JAVA_OPTS: "-Xms512m -Xmx1g"
  ulimits:
    memlock:
      soft: -1
      hard: -1
  volumes:
    - ./data/elasticsearch:/usr/share/elasticsearch/data
    - ./volumes/elasticsearch/analysis-smartcn.zip:/tmp/analysis-smartcn.zip
  command: >
    bash -c "
      if [ ! -f /usr/share/elasticsearch/plugins/analysis-smartcn/plugin-descriptor.properties ]; then
        echo 'Installing SmartCN plugin...'
        /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///tmp/analysis-smartcn.zip --batch
      fi
      /usr/local/bin/docker-entrypoint.sh eswrapper
    "
  ports:
    - "9200:9200"
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 40s
```

### ç´¢å¼•æ¨¡æ¿è®¾è®¡

```json
{
  "template": {
    "settings": {
      "analysis": {
        "analyzer": {
          "smartcn_analyzer": {
            "type": "custom",
            "tokenizer": "smartcn_tokenizer",
            "filter": ["lowercase", "stop"]
          },
          "ik_smart_analyzer": {
            "type": "custom", 
            "tokenizer": "ik_smart",
            "filter": ["lowercase"]
          }
        }
      },
      "index": {
        "number_of_shards": 1,
        "number_of_replicas": 1,
        "refresh_interval": "5s"
      }
    },
    "mappings": {
      "properties": {
        "slice_id": {
          "type": "keyword"
        },
        "knowledge_id": {
          "type": "long"
        },
        "document_id": {
          "type": "long"
        },
        "title": {
          "type": "text",
          "analyzer": "smartcn_analyzer",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "content": {
          "type": "text",
          "analyzer": "smartcn_analyzer",
          "fields": {
            "ik": {
              "type": "text",
              "analyzer": "ik_smart_analyzer"
            }
          }
        },
        "metadata": {
          "type": "object",
          "enabled": true
        },
        "created_at": {
          "type": "date",
          "format": "epoch_millis"
        },
        "updated_at": {
          "type": "date", 
          "format": "epoch_millis"
        }
      }
    }
  }
}
```

### å…¨æ–‡æ£€ç´¢å®ç°

```go
// ES ç´¢å¼•å®ç°
func (e *elasticsearchManager) Index(ctx context.Context, request *indexer.IndexRequest) error {
    indexName := request.Options["index_name"].(string)
    
    // 1. ç¡®ä¿ç´¢å¼•å­˜åœ¨
    exists, err := e.client.Indices.Exists([]string{indexName})
    if err != nil {
        return err
    }
    
    if !exists.StatusCode == 200 {
        if err := e.createIndex(ctx, indexName); err != nil {
            return err
        }
    }
    
    // 2. æ‰¹é‡ç´¢å¼•æ–‡æ¡£
    var buf bytes.Buffer
    for _, doc := range request.Documents {
        // æ„å»ºç´¢å¼•æ“ä½œ
        meta := map[string]interface{}{
            "index": map[string]interface{}{
                "_index": indexName,
                "_id":    doc.ID,
            },
        }
        
        // æ„å»ºæ–‡æ¡£å†…å®¹
        docContent := map[string]interface{}{
            "slice_id":     doc.ID,
            "knowledge_id": doc.Metadata["knowledge_id"],
            "document_id":  doc.Metadata["document_id"],
            "title":        doc.Metadata["title"],
            "content":      doc.Content,
            "metadata":     doc.Metadata,
            "created_at":   time.Now().UnixMilli(),
            "updated_at":   time.Now().UnixMilli(),
        }
        
        // åºåˆ—åŒ–åˆ°æ‰¹é‡æ“ä½œç¼“å†²åŒº
        if err := json.NewEncoder(&buf).Encode(meta); err != nil {
            return err
        }
        if err := json.NewEncoder(&buf).Encode(docContent); err != nil {
            return err
        }
    }
    
    // 3. æ‰§è¡Œæ‰¹é‡ç´¢å¼•
    res, err := e.client.Bulk(
        &buf,
        e.client.Bulk.WithIndex(indexName),
        e.client.Bulk.WithRefresh("true"),
    )
    
    if err != nil {
        return fmt.Errorf("bulk index failed: %w", err)
    }
    defer res.Body.Close()
    
    return e.handleBulkResponse(res)
}

// ES æ£€ç´¢å®ç°
func (e *elasticsearchManager) Retrieve(ctx context.Context, request *retriever.RetrieveRequest) (*retriever.RetrieveResponse, error) {
    indexName := request.Options["index_name"].(string)
    
    // 1. æ„å»ºæœç´¢æŸ¥è¯¢
    query := map[string]interface{}{
        "query": map[string]interface{}{
            "bool": map[string]interface{}{
                "should": []interface{}{
                    // ç²¾ç¡®çŸ­è¯­åŒ¹é…
                    map[string]interface{}{
                        "match_phrase": map[string]interface{}{
                            "content": map[string]interface{}{
                                "query": request.Query,
                                "boost": 3.0,
                            },
                        },
                    },
                    // æ¨¡ç³ŠåŒ¹é…
                    map[string]interface{}{
                        "match": map[string]interface{}{
                            "content": map[string]interface{}{
                                "query":     request.Query,
                                "boost":     2.0,
                                "fuzziness": "AUTO",
                                "operator":  "and",
                            },
                        },
                    },
                    // å¤šå­—æ®µåŒ¹é…
                    map[string]interface{}{
                        "multi_match": map[string]interface{}{
                            "query":  request.Query,
                            "fields": []string{"title^2", "content"},
                            "boost":  1.0,
                        },
                    },
                },
                "minimum_should_match": 1,
            },
        },
        "size": request.TopK,
        "highlight": map[string]interface{}{
            "fields": map[string]interface{}{
                "content": map[string]interface{}{
                    "fragment_size": 150,
                    "number_of_fragments": 3,
                },
            },
        },
        "sort": []interface{}{
            map[string]interface{}{
                "_score": map[string]interface{}{
                    "order": "desc",
                },
            },
        },
    }
    
    // 2. æ‰§è¡Œæœç´¢
    var buf bytes.Buffer
    if err := json.NewEncoder(&buf).Encode(query); err != nil {
        return nil, err
    }
    
    res, err := e.client.Search(
        e.client.Search.WithContext(ctx),
        e.client.Search.WithIndex(indexName),
        e.client.Search.WithBody(&buf),
    )
    
    if err != nil {
        return nil, fmt.Errorf("search failed: %w", err)
    }
    defer res.Body.Close()
    
    // 3. è§£ææœç´¢ç»“æœ
    return e.parseSearchResponse(res)
}
```

## ğŸ“¦ å¯¹è±¡å­˜å‚¨å®ç°

### MinIO é…ç½®

**Docker é…ç½®ä½ç½®**: `docker/docker-compose.yml`

```yaml
minio:
  image: bitnami/minio:2025.1.18
  container_name: coze-minio
  restart: always
  environment:
    MINIO_ROOT_USER: minioadmin
    MINIO_ROOT_PASSWORD: minioadmin
    MINIO_DEFAULT_BUCKETS: coze-studio
    MINIO_SCHEME: http
  volumes:
    - ./data/minio:/bitnami/minio/data
  ports:
    - "9000:9000"
    - "9001:9001"
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1"]
    interval: 30s
    timeout: 20s
    retries: 3
    start_period: 30s
```

### å¯¹è±¡å­˜å‚¨æ¥å£

**ä½ç½®**: `backend/infra/storage/`

```go
// å­˜å‚¨æ¥å£
type Storage interface {
    // ä¸Šä¼ å¯¹è±¡
    PutObject(ctx context.Context, key string, data []byte, options ...Option) error
    
    // è·å–å¯¹è±¡
    GetObject(ctx context.Context, key string) ([]byte, error)
    
    // è·å–å¯¹è±¡URL (å¸¦ç­¾å)
    GetObjectUrl(ctx context.Context, key string, options ...Option) (string, error)
    
    // åˆ é™¤å¯¹è±¡
    DeleteObject(ctx context.Context, key string) error
    
    // åˆ—å‡ºå¯¹è±¡
    ListObjects(ctx context.Context, prefix string) ([]ObjectInfo, error)
    
    // æ£€æŸ¥å¯¹è±¡æ˜¯å¦å­˜åœ¨
    ObjectExists(ctx context.Context, key string) (bool, error)
}

// å­˜å‚¨é€‰é¡¹
type Option func(*Options)

type Options struct {
    ContentType string
    Expire      int64
    Metadata    map[string]string
}

// å¯¹è±¡ä¿¡æ¯
type ObjectInfo struct {
    Key          string
    Size         int64
    ContentType  string
    LastModified time.Time
    ETag         string
}
```

### æ–‡ä»¶å­˜å‚¨ç­–ç•¥

```go
// MinIO å­˜å‚¨å®ç°
type minioStorage struct {
    client     *minio.Client
    bucketName string
    config     Config
}

type Config struct {
    Endpoint        string
    AccessKeyID     string
    SecretAccessKey string
    BucketName      string
    UseSSL          bool
    Region          string
}

// ä¸Šä¼ æ–‡ä»¶å®ç°
func (m *minioStorage) PutObject(ctx context.Context, key string, data []byte, options ...Option) error {
    opts := &Options{}
    for _, option := range options {
        option(opts)
    }
    
    // è®¾ç½®é»˜è®¤å†…å®¹ç±»å‹
    contentType := opts.ContentType
    if contentType == "" {
        contentType = detectContentType(key, data)
    }
    
    // æ„å»ºä¸Šä¼ é€‰é¡¹
    putOptions := minio.PutObjectOptions{
        ContentType:  contentType,
        UserMetadata: opts.Metadata,
    }
    
    // æ‰§è¡Œä¸Šä¼ 
    reader := bytes.NewReader(data)
    _, err := m.client.PutObject(ctx, m.bucketName, key, reader, int64(len(data)), putOptions)
    
    if err != nil {
        return fmt.Errorf("failed to put object %s: %w", key, err)
    }
    
    return nil
}

// è·å–ç­¾åURLå®ç°
func (m *minioStorage) GetObjectUrl(ctx context.Context, key string, options ...Option) (string, error) {
    opts := &Options{
        Expire: 3600, // é»˜è®¤1å°æ—¶è¿‡æœŸ
    }
    for _, option := range options {
        option(opts)
    }
    
    // ç”Ÿæˆé¢„ç­¾åURL
    presignedURL, err := m.client.PresignedGetObject(
        ctx,
        m.bucketName,
        key,
        time.Duration(opts.Expire)*time.Second,
        nil,
    )
    
    if err != nil {
        return "", fmt.Errorf("failed to generate presigned URL for %s: %w", key, err)
    }
    
    return presignedURL.String(), nil
}

// å†…å®¹ç±»å‹æ£€æµ‹
func detectContentType(key string, data []byte) string {
    // 1. æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­
    ext := strings.ToLower(filepath.Ext(key))
    switch ext {
    case ".pdf":
        return "application/pdf"
    case ".docx":
        return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    case ".xlsx":
        return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    case ".jpg", ".jpeg":
        return "image/jpeg"
    case ".png":
        return "image/png"
    case ".txt":
        return "text/plain"
    case ".md":
        return "text/markdown"
    }
    
    // 2. æ ¹æ®æ–‡ä»¶å†…å®¹åˆ¤æ–­
    contentType := http.DetectContentType(data)
    if contentType != "application/octet-stream" {
        return contentType
    }
    
    // 3. é»˜è®¤ç±»å‹
    return "application/octet-stream"
}
```

### æ–‡ä»¶ç»„ç»‡ç­–ç•¥

```go
// æ–‡ä»¶è·¯å¾„ç”Ÿæˆç­–ç•¥
type FilePathGenerator struct {
    baseWord string
}

func NewFilePathGenerator() *FilePathGenerator {
    return &FilePathGenerator{
        baseWord: "1Aa2Bb3Cc4Dd5Ee6Ff7Gg8Hh9Ii0JjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz",
    }
}

// ç”Ÿæˆæ–‡ä»¶URI
func (g *FilePathGenerator) GenerateURI(userID int64, fileType string, bizType string) string {
    // 1. ç”Ÿæˆéšæœºæ ‡è¯†ç¬¦
    input := fmt.Sprintf("upload_%d_Ma*9)fhi_%d_gou_%s_rand_%d", 
        userID, time.Now().Unix(), fileType, rand.Intn(100000))
    
    // 2. è®¡ç®—å“ˆå¸Œ
    hash := sha256.Sum256([]byte(input))
    hashString := base64.StdEncoding.EncodeToString(hash[:])
    
    // 3. è½¬æ¢ä¸ºè‡ªå®šä¹‰å­—ç¬¦é›†
    secret := ""
    for _, char := range hashString[:10] {
        index := int(char) % 62
        secret += string(g.baseWord[index])
    }
    
    // 4. æ„å»ºæœ€ç»ˆè·¯å¾„
    suffix := fmt.Sprintf("%d_%d_%s.%s", userID, time.Now().UnixNano(), secret, fileType)
    return fmt.Sprintf("%s/%s", bizType, suffix)
}

// ä¸šåŠ¡ç±»å‹å¸¸é‡
const (
    BizTypeBotDataset   = "bot_dataset"     // æœºå™¨äººçŸ¥è¯†åº“
    BizTypeUserUpload   = "user_upload"     // ç”¨æˆ·ä¸Šä¼ 
    BizTypeDocReview    = "doc_review"      // æ–‡æ¡£å®¡æ ¸
    BizTypeImagePreview = "image_preview"   // å›¾ç‰‡é¢„è§ˆ
)
```

## ğŸ—„ï¸ å…³ç³»æ•°æ®åº“å®ç°

### MySQL è¡¨ç»“æ„è®¾è®¡

**Schema ä½ç½®**: `docker/volumes/mysql/schema.sql`

```sql
-- çŸ¥è¯†åº“è¡¨
CREATE TABLE knowledge (
    id BIGINT PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    icon_uri VARCHAR(512),
    creator_id BIGINT NOT NULL,
    space_id BIGINT NOT NULL,
    app_id BIGINT,
    format_type INT NOT NULL DEFAULT 1, -- 1:æ–‡æœ¬ 2:è¡¨æ ¼ 3:å›¾ç‰‡
    status INT NOT NULL DEFAULT 1,
    created_at BIGINT NOT NULL,
    updated_at BIGINT NOT NULL,
    INDEX idx_creator_space (creator_id, space_id),
    INDEX idx_app_id (app_id),
    INDEX idx_created_at (created_at)
);

-- çŸ¥è¯†åº“æ–‡æ¡£è¡¨
CREATE TABLE knowledge_document (
    id BIGINT PRIMARY KEY,
    knowledge_id BIGINT NOT NULL,
    name VARCHAR(255) NOT NULL,
    uri VARCHAR(512) NOT NULL,
    file_extension VARCHAR(32),
    document_type INT NOT NULL,
    size BIGINT DEFAULT 0,
    slice_count INT DEFAULT 0,
    char_count BIGINT DEFAULT 0,
    creator_id BIGINT NOT NULL,
    space_id BIGINT NOT NULL,
    source_type INT NOT NULL DEFAULT 1,
    status INT NOT NULL DEFAULT 1,
    fail_reason TEXT,
    parse_rule JSON,  -- è§£æè§„åˆ™
    table_info JSON,  -- è¡¨æ ¼ä¿¡æ¯
    created_at BIGINT NOT NULL,
    updated_at BIGINT NOT NULL,
    INDEX idx_knowledge_id (knowledge_id),
    INDEX idx_creator_space (creator_id, space_id),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
);

-- çŸ¥è¯†åº“æ–‡æ¡£ç‰‡æ®µè¡¨
CREATE TABLE knowledge_document_slice (
    id BIGINT PRIMARY KEY,
    knowledge_id BIGINT NOT NULL,
    document_id BIGINT NOT NULL,
    content TEXT,
    sequence DOUBLE DEFAULT 0,
    hit BIGINT DEFAULT 0,
    creator_id BIGINT NOT NULL,
    space_id BIGINT NOT NULL,
    status INT NOT NULL DEFAULT 1,
    created_at BIGINT NOT NULL,
    updated_at BIGINT NOT NULL,
    INDEX idx_knowledge_id (knowledge_id),
    INDEX idx_document_id (document_id),
    INDEX idx_sequence (sequence),
    INDEX idx_creator_space (creator_id, space_id),
    INDEX idx_created_at (created_at)
);
```

### åŠ¨æ€è¡¨ç®¡ç†

```go
// RDB æ¥å£å®šä¹‰
type RDB interface {
    // è¡¨ç®¡ç†
    CreateTable(ctx context.Context, req *CreateTableRequest) (*CreateTableResponse, error)
    DropTable(ctx context.Context, req *DropTableRequest) (*DropTableResponse, error)
    
    // æ•°æ®æ“ä½œ
    InsertData(ctx context.Context, req *InsertDataRequest) (*InsertDataResponse, error)
    QueryData(ctx context.Context, req *QueryDataRequest) (*QueryDataResponse, error)
    UpdateData(ctx context.Context, req *UpdateDataRequest) (*UpdateDataResponse, error)
    DeleteData(ctx context.Context, req *DeleteDataRequest) (*DeleteDataResponse, error)
}

// åŠ¨æ€å»ºè¡¨å®ç°
func (r *mysqlRDB) CreateTable(ctx context.Context, req *CreateTableRequest) (*CreateTableResponse, error) {
    // 1. ç”Ÿæˆè¡¨å
    tableName := r.generateTableName()
    
    // 2. æ„å»ºCREATE TABLEè¯­å¥
    var columns []string
    var indexes []string
    
    for _, col := range req.Table.Columns {
        columnDef := fmt.Sprintf("`%s` %s", col.Name, convertDataType(col.DataType))
        if col.NotNull {
            columnDef += " NOT NULL"
        }
        if col.DefaultValue != "" {
            columnDef += fmt.Sprintf(" DEFAULT %s", col.DefaultValue)
        }
        columns = append(columns, columnDef)
    }
    
    // 3. å¤„ç†ç´¢å¼•
    for _, idx := range req.Table.Indexes {
        switch idx.Type {
        case entity.PrimaryKey:
            indexes = append(indexes, fmt.Sprintf("PRIMARY KEY (%s)", 
                strings.Join(quoteColumns(idx.Columns), ", ")))
        case entity.UniqueKey:
            indexes = append(indexes, fmt.Sprintf("UNIQUE KEY `%s` (%s)", 
                idx.Name, strings.Join(quoteColumns(idx.Columns), ", ")))
        case entity.Index:
            indexes = append(indexes, fmt.Sprintf("KEY `%s` (%s)", 
                idx.Name, strings.Join(quoteColumns(idx.Columns), ", ")))
        }
    }
    
    // 4. æ‰§è¡Œå»ºè¡¨SQL
    createSQL := fmt.Sprintf("CREATE TABLE `%s` (\n  %s\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci",
        tableName, strings.Join(append(columns, indexes...), ",\n  "))
    
    _, err := r.db.ExecContext(ctx, createSQL)
    if err != nil {
        return nil, fmt.Errorf("create table failed: %w", err)
    }
    
    return &CreateTableResponse{
        Table: &entity.Table{
            Name:    tableName,
            Columns: req.Table.Columns,
            Indexes: req.Table.Indexes,
        },
    }, nil
}

// è¡¨åç”Ÿæˆç­–ç•¥
func (r *mysqlRDB) generateTableName() string {
    timestamp := time.Now().Unix()
    randomNum := rand.Intn(1000000)
    return fmt.Sprintf("table_%d_%d", timestamp, randomNum)
}
```

## ğŸ’¾ ç¼“å­˜å±‚å®ç°

### Redis é…ç½®

**Docker é…ç½®ä½ç½®**: `docker/docker-compose.yml`

```yaml
redis:
  image: bitnami/redis:8.0
  container_name: coze-redis
  restart: always
  environment:
    - REDIS_AOF_ENABLED=no
    - REDIS_PORT_NUMBER=6379
    - REDIS_IO_THREADS=4
    - ALLOW_EMPTY_PASSWORD=yes
  volumes:
    - ./data/bitnami/redis:/bitnami/redis/data:rw,Z
  ports:
    - "6379:6379"
```

### ç¼“å­˜ç­–ç•¥å®ç°

```go
// ç¼“å­˜ç®¡ç†å™¨
type CacheManager struct {
    client cache.Cmdable
    config CacheConfig
}

type CacheConfig struct {
    DefaultExpire time.Duration
    LongExpire    time.Duration
    ShortExpire   time.Duration
}

// å¤šçº§ç¼“å­˜å®ç°
func (c *CacheManager) GetWithFallback(ctx context.Context, key string, fallback func() (interface{}, error)) (interface{}, error) {
    // 1. å°è¯•ä»ç¼“å­˜è·å–
    result := c.client.Get(ctx, key)
    if result.Err() == nil {
        var data interface{}
        if err := json.Unmarshal([]byte(result.Val()), &data); err == nil {
            return data, nil
        }
    }
    
    // 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨å›è°ƒå‡½æ•°
    data, err := fallback()
    if err != nil {
        return nil, err
    }
    
    // 3. è®¾ç½®ç¼“å­˜
    jsonData, _ := json.Marshal(data)
    c.client.Set(ctx, key, jsonData, c.config.DefaultExpire)
    
    return data, nil
}

// æ‰¹é‡ç¼“å­˜æ“ä½œ
func (c *CacheManager) MGetWithFallback(ctx context.Context, keys []string, fallback func([]string) (map[string]interface{}, error)) (map[string]interface{}, error) {
    result := make(map[string]interface{})
    var missedKeys []string
    
    // 1. æ‰¹é‡è·å–ç¼“å­˜
    if len(keys) > 0 {
        cached := c.client.MGet(ctx, keys...)
        for i, key := range keys {
            if i < len(cached.Val()) && cached.Val()[i] != nil {
                var data interface{}
                if err := json.Unmarshal([]byte(cached.Val()[i].(string)), &data); err == nil {
                    result[key] = data
                } else {
                    missedKeys = append(missedKeys, key)
                }
            } else {
                missedKeys = append(missedKeys, key)
            }
        }
    }
    
    // 2. å¤„ç†ç¼“å­˜æœªå‘½ä¸­çš„é”®
    if len(missedKeys) > 0 {
        fallbackData, err := fallback(missedKeys)
        if err != nil {
            return nil, err
        }
        
        // 3. æ‰¹é‡è®¾ç½®ç¼“å­˜
        pipeline := c.client.Pipeline()
        for key, data := range fallbackData {
            jsonData, _ := json.Marshal(data)
            pipeline.Set(ctx, key, jsonData, c.config.DefaultExpire)
            result[key] = data
        }
        pipeline.Exec(ctx)
    }
    
    return result, nil
}
```

## ğŸ”§ å­˜å‚¨ä¼˜åŒ–ç­–ç•¥

### æ•°æ®åˆ†åŒºç­–ç•¥

```go
// çŸ¥è¯†åº“æ•°æ®åˆ†åŒº
func getCollectionName(knowledgeID int64) string {
    return fmt.Sprintf("knowledge_%d", knowledgeID)
}

func getIndexName(knowledgeID int64) string {
    return fmt.Sprintf("knowledge_doc_%d", knowledgeID)
}

// æ—¶é—´åˆ†åŒºç­–ç•¥ (æ—¥å¿—è¡¨)
func getLogTableName(date time.Time) string {
    return fmt.Sprintf("retrieval_log_%s", date.Format("200601"))
}
```

### å­˜å‚¨å‹ç¼©ç­–ç•¥

```go
// è¡¨æ ¼æ•°æ®å‹ç¼©
func (k *knowledgeSVC) compressTableData(data []map[string]interface{}) ([]byte, error) {
    if !k.enableCompactTable {
        return json.Marshal(data)
    }
    
    // 1. JSONåºåˆ—åŒ–
    jsonData, err := json.Marshal(data)
    if err != nil {
        return nil, err
    }
    
    // 2. Gzipå‹ç¼©
    var buf bytes.Buffer
    gzipWriter := gzip.NewWriter(&buf)
    
    if _, err := gzipWriter.Write(jsonData); err != nil {
        return nil, err
    }
    
    if err := gzipWriter.Close(); err != nil {
        return nil, err
    }
    
    return buf.Bytes(), nil
}

// è¡¨æ ¼æ•°æ®è§£å‹
func (k *knowledgeSVC) decompressTableData(compressedData []byte) ([]map[string]interface{}, error) {
    if !k.enableCompactTable {
        var data []map[string]interface{}
        err := json.Unmarshal(compressedData, &data)
        return data, err
    }
    
    // 1. Gzipè§£å‹
    reader := bytes.NewReader(compressedData)
    gzipReader, err := gzip.NewReader(reader)
    if err != nil {
        return nil, err
    }
    defer gzipReader.Close()
    
    jsonData, err := io.ReadAll(gzipReader)
    if err != nil {
        return nil, err
    }
    
    // 2. JSONååºåˆ—åŒ–
    var data []map[string]interface{}
    err = json.Unmarshal(jsonData, &data)
    return data, err
}
```

## ğŸ“Š ç›‘æ§å’Œè¿ç»´

### å­˜å‚¨ç›‘æ§æŒ‡æ ‡

```go
// å­˜å‚¨æ€§èƒ½æŒ‡æ ‡
type StorageMetrics struct {
    // Milvus æŒ‡æ ‡
    VectorInsertQPS     float64
    VectorSearchQPS     float64 
    VectorSearchLatency time.Duration
    CollectionCount     int64
    VectorCount         int64
    
    // Elasticsearch æŒ‡æ ‡  
    ESIndexQPS          float64
    ESSearchQPS         float64
    ESSearchLatency     time.Duration
    IndexSize           int64
    DocumentCount       int64
    
    // MinIO æŒ‡æ ‡
    ObjectUploadQPS     float64
    ObjectDownloadQPS   float64
    StorageUsage        int64
    ObjectCount         int64
    
    // MySQL æŒ‡æ ‡
    ConnectionCount     int64
    SlowQueryCount      int64
    TableSize           int64
    
    // Redis æŒ‡æ ‡
    CacheHitRate        float64
    MemoryUsage         int64
    ConnectionCount     int64
}
```

### å­˜å‚¨å¥åº·æ£€æŸ¥

```go
func (s *StorageManager) HealthCheck(ctx context.Context) error {
    var errs []error
    
    // æ£€æŸ¥ Milvus
    if err := s.checkMilvusHealth(ctx); err != nil {
        errs = append(errs, fmt.Errorf("milvus health check failed: %w", err))
    }
    
    // æ£€æŸ¥ Elasticsearch
    if err := s.checkElasticsearchHealth(ctx); err != nil {
        errs = append(errs, fmt.Errorf("elasticsearch health check failed: %w", err))
    }
    
    // æ£€æŸ¥ MinIO
    if err := s.checkMinIOHealth(ctx); err != nil {
        errs = append(errs, fmt.Errorf("minio health check failed: %w", err))
    }
    
    // æ£€æŸ¥ MySQL
    if err := s.checkMySQLHealth(ctx); err != nil {
        errs = append(errs, fmt.Errorf("mysql health check failed: %w", err))
    }
    
    // æ£€æŸ¥ Redis
    if err := s.checkRedisHealth(ctx); err != nil {
        errs = append(errs, fmt.Errorf("redis health check failed: %w", err))
    }
    
    if len(errs) > 0 {
        return fmt.Errorf("storage health check failed: %v", errs)
    }
    
    return nil
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-27  
**ç›¸å…³æ–‡æ¡£**: [çŸ¥è¯†åº“æ¶æ„æ€»è§ˆ](./knowledge-base-architecture.md)